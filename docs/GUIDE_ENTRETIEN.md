# üéì Guide d'Entretien : Projet Sentinelle-Plateforme

Ce document est con√ßu pour t'aider √† pr√©senter **Sentinelle** lors d'un entretien technique ou fonctionnel. Il d√©cortique chaque aspect du projet avec un angle p√©dagogique ("Pourquoi on a fait √ßa ?") et technique ("Comment √ßa marche ?").

---

## 1. Le Pitch (L'accroche) üé§

**"C'est quoi Sentinelle ?"**

> "Sentinelle-Plateforme est une plateforme de d√©cision temps r√©el pour l'√©valuation du risque de cr√©dit et la d√©tection de fraude. C'est une solution 'Enterprise-Ready' qui combine l'analyse quantitative classique (Scoring) avec l'analyse comportementale (Fraude) et l'IA G√©n√©rative pour l'explicabilit√©."

**Mots-cl√©s √† placer :**
*   **Temps r√©el** (API < 200ms)
*   **Hybride** (R√®gles m√©tiers + Machine Learning)
*   **Human-in-the-loop** (L'IA propose, l'humain valide les cas complexes)
*   **Explicabilit√©** (SHAP + LLM)
*   **Conformit√©** (RGPD / AI Act)

---

## 2. Architecture Technique (La vue d'ensemble) üèóÔ∏è

Si on te demande de dessiner l'architecture, visualise ces 4 blocs :

1.  **L'API Gateway (FastAPI)** : C'est le chef d'orchestre. Elle re√ßoit la demande, interroge les mod√®les, v√©rifie les r√®gles, et stocke le r√©sultat.
2.  **Le Moteur ML (Scikit-Learn / XGBoost)** :
    *   Un mod√®le supervis√© pour le **Cr√©dit** (est-ce qu'il va payer ?).
    *   Un mod√®le non-supervis√© pour la **Fraude** (est-ce une anomalie ?).
3.  **L'Agent Explicatif (LLM)** : Il prend les scores bruts et les valeurs SHAP pour r√©diger un rapport en fran√ßais ("Le client est rejet√© car son ratio dette/revenu est trop √©lev√©...").
4.  **L'Observabilit√© (Grafana/Prometheus)** :
    *   **Monitoring "Senior++" (IaC)** : On ne configure rien √† la main. Tout est code.
    *   **Producer** : L'API expose des m√©triques m√©tier (`decision_total_count`, `model_latency`) via Prometheus Client.
    *   **Scraper** : Prometheus collecte ces donn√©es toutes les 5s.
    *   **Viewer** : Grafana est pr√©-configur√© (provisioning) pour afficher un dashboard JSON versionn√©. Cela garantit que si on red√©ploie l'infra, on ne perd pas nos graphiques.

---

## 3. Choix Techniques & Justifications üí°

En entretien, on te demandera "Pourquoi X et pas Y ?". Voici les r√©ponses :

### üêç Pourquoi Python & FastAPI ?
*   **Performance** : FastAPI est asynchrone, parfait pour g√©rer beaucoup de requ√™tes simultan√©es.
*   **Typage fort (Pydantic)** : On valide les donn√©es √† l'entr√©e. Si l'√¢ge est une cha√Æne de caract√®res ("trente"), √ßa plante proprement avant m√™me d'arriver au mod√®le. S√©curit√© et robustesse.

### ü§ñ Pourquoi ces mod√®les ML ?
*   **Pour le Cr√©dit (Logistic Regression / XGBoost)** : On a besoin d'**interpr√©tabilit√©**. Une banque doit pouvoir dire pourquoi un cr√©dit est refus√©. La r√©gression logistique est transparente.
*   **Pour la Fraude (Isolation Forest)** : La fraude change tout le temps. Un mod√®le supervis√© apprendrait "les fraudes d'hier". L'*Isolation Forest* d√©tecte les **anomalies** (ce qui sort de l'ordinaire), ce qui est plus robuste pour d√©tecter les *nouveaux* types de fraudes.

### ‚öñÔ∏è Pourquoi SHAP (Shapley Additive Explanations) ?
*   C'est le standard de l'industrie pour l'explicabilit√©. √áa nous dit *exactement* combien chaque variable (revenu, √¢ge, dette) a contribu√© au score final, positivement ou n√©gativement.

### üê≥ Pourquoi Docker ?
*   Pour la **reproductibilit√©**. "√áa marche sur ma machine" n'est pas une r√©ponse acceptable. Avec Docker, l'environnement est iso-prod du d√©but √† la fin.

---

## 4. Les Challenges R√©solus (Pour briller) ‚ú®

Raconte une histoire sur les difficult√©s rencontr√©es :

*   **Le Challenge de l'Explicabilit√©** : "Les scores bruts (0.76) ne parlent pas aux conseillers bancaires. J'ai int√©gr√© un **Agent LLM** qui traduit ces maths en phrases claires en fran√ßais. J'ai d√ª travailler le *Prompt Engineering* pour que l'IA n'hallucine pas (ne pas inventer de chiffres)."
*   **La Gestion des Cas Limites** : "L'automatisation √† 100% est dangereuse. J'ai impl√©ment√© une logique de 'Zone Grise' (Review). Si le score est moyen, on ne rejette pas automatiquement, on envoie √† un humain. C'est crucial pour l'√©thique et la conformit√©."
*   **L'Interface Utilisateur** : "Je ne voulais pas d'un Swagger gris. J'ai d√©velopp√© un Dashboard complet (Th√®me Light Corporate / Dark Mode) pour que les parties prenantes non-techniques puissent tester et visualiser les d√©cisions."

---

## 5. Conformit√© & √âthique (Le bonus Senior) üõ°Ô∏è

Montre que tu ne fais pas que du code, mais que tu comprends le m√©tier :

*   **RGPD** : "J'ai appliqu√© la **pseudonymisation**. On ne stocke pas le nom du client, mais un hash de son ID."
*   **AI Act** : "Le syst√®me est document√© (Model Card, Data Sheet). Il y a toujours un humain dans la boucle pour les d√©cisions critiques."

---

## 6. Questions Types auxquelles s'attendre ‚ùì

**Q: Comment g√®res-tu le r√©-entra√Ænement du mod√®le ?**
*   **R:** "Actuellement c'est un script manuel, mais gr√¢ce √† MLflow, on tracke toutes les m√©triques. L'√©tape suivante serait d'automatiser le r√©-entra√Ænement via Airflow si on d√©tecte une d√©rive (Data Drift) dans Grafana."

**Q: Si l'API est lente, que fais-tu ?**
*   **R:** "Je regarde d'abord les m√©triques Prometheus pour voir si c'est le mod√®le ML ou la base de donn√©es qui bloque. Si c'est le mod√®le, je peux optimiser l'inf√©rence (ONNX) ou scaler horizontalement les conteneurs API avec Kubernetes."

**Q: Comment g√®res-tu le monitoring en production ?**
*   **R:** "Le monitoring est enti√®rement **provisionn√© as code**. Les m√©triques m√©tier sont expos√©es par l'API, scrapp√©es par Prometheus, et le dashboard Grafana est versionn√© en JSON. C'est reproductible, audit-able et iso-prod imm√©diat."

**Q: Comment s√©curises-tu l'application ?**
*   **R:** "Validation stricte des entr√©es (Pydantic), pas de donn√©es sensibles en clair (Hash), et isolation des services via Docker Network."
